<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Abnormal Distributions</title>
    <link>https://abndistro.com/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on Abnormal Distributions</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://abndistro.com/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Basic usage of GluonTS for probabilistic, deep learning-based time series forecasting</title>
      <link>https://abndistro.com/post/2021/05/12/basic-usage-of-gluonts-for-time-series-forecasting-in-python/</link>
      <pubDate>Wed, 12 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://abndistro.com/post/2021/05/12/basic-usage-of-gluonts-for-time-series-forecasting-in-python/</guid>
      <description>View notebook in GitHub - https://github.com/dannymorris/abndistro2/blob/master/notebooks/gluonts.ipynb
Overview GluonTS is a deep learning-based framework for probabilisitc time series modeling and forecasting. As mentioned in the paper, deep learning models improve upon traditional univariate models (e.g.Â Arima) with their ability to train a single, global model over an entire collection of time series.
 Takeaways from my experience getting started  Training and inference for all 152 time series required about 2 minutes in total using the DeepAR estimator and CPUs provided by Google Colab.</description>
    </item>
    
    <item>
      <title>My top 6% solution in the Kaggle M5 Forecasting - Accuracy competition using a machine learning ensemble</title>
      <link>https://abndistro.com/post/2020/07/08/my-top-6-solution-in-the-kaggle-m5-forecasting-accuracy-competition-using-a-machine-learning-ensemble/</link>
      <pubDate>Wed, 08 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://abndistro.com/post/2020/07/08/my-top-6-solution-in-the-kaggle-m5-forecasting-accuracy-competition-using-a-machine-learning-ensemble/</guid>
      <description>Overview Competition overview Forecasting as a supervised machine learning problem General feature engineering Models  One model per product using Random Forests One model per store using XGBoost Model performance with and without ensembling  Summary of my development and evaluation strategy Potential improvements Conclusion   Overview I participated in the M5 Forecasting - Accuracy Kaggle competition, in which the goal was to submit daily forecasts for over 30,000 Walmart products.</description>
    </item>
    
    <item>
      <title>A supervised learning strategy for detecting rare classes using data-centric ensembling</title>
      <link>https://abndistro.com/post/2020/06/07/a-supervised-learning-strategy-for-detecting-rare-classes-using-data-centric-ensembling/</link>
      <pubDate>Sun, 07 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://abndistro.com/post/2020/06/07/a-supervised-learning-strategy-for-detecting-rare-classes-using-data-centric-ensembling/</guid>
      <description>Overview A little bit about ensembling Defining the strategy Testing the strategy Pseudocode for training and testing Pseudocode for cross validation Pseudocode for production   Overview This post demonstrates a strategy for rare class learning whereby out-of-sample predictions are made using data-centric ensembling. This strategy is described by Charu Aggarwal in section 7.2 of his book Outlier Analysis. This is an excellent book that presents a ton of interesting material on statistics and modeling in a very readable way.</description>
    </item>
    
  </channel>
</rss>
